Description:

Package: nira.texmin
Type: Package
Title: What the package does (short line)
Version: 1.0
Date: 2016-01-19
Author: Who wrote it
Maintainer: Who to complain to <yourfault@somewhere.net>
Description: More about what it does (maybe more than one line)
License: What license is it under?

nira.texmin:

#' nira.texmin: This package is a tool-box for text mining.
#'
#' @section Class TEXT.MINER:
#' nira.texmin provides a Reference class named as TEXT.MINER.
#' This package supports the latest techniques of text mining and sentiment analysis.
#' It also supports various visualizations for presentation.
#'
#' @docType package
#' @name nira.texmin
#' 
# @include 

NULL
#> NULL

text.miner:

# Header
# Filename:     text.miner.R
# Description:  A reference R class capable of running various text mining analysis with visualization.
# Author:       Nima Ramezani Taghiabadi
# Email :       Nima.Ramezani@cba.com.au
# Start Date:   20 November 2015
# Last change:  10 March 2016
# Version:      3.1

# Changes to version 3.0:
# 
#  1- Modified to accommodate niragen version 1.2
# Todo:
#  1- similarly change function set.weighting() to a setter method
#  2- write a setValidity() method
#  3- add a coercion method


# 
# lib.set = c()
# lib.set = c(lib.set, paste(packages.path, "general_lib.R", sep = "/"))
# lib.set = c(lib.set, paste(packages.path, "mathematics", "linear_algebra.R", sep = "/"))
# lib.set = c(lib.set, paste(packages.path, "artificial_intelligence","data_mining","text_mining", "text_mining_lib_v2.R", sep = "/"))
# 

metrics    = c("euclidean", "maximum", "manhattan", "canberra", "binary" , "minkowski", "spherical")
weightings = c("tfidf", "freq")

plural.col = function(colour){
  #   "BrBG"     "PiYG"     "PRGn"     "PuOr"     "RdBu"    
  #   "RdGy"     "RdYlBu"   "RdYlGn"   "Spectral" "Accent"  
  #   "Dark2"    "Paired"   "Pastel1"  "Pastel2"  "Set1"    
  #   "Set2"     "Set3"     "Blues"    "BuGn"     "BuPu"    
  #   "GnBu"     "Greens"   "Greys"    "Oranges"  "OrRd"    
  #   "PuBu"     "PuBuGn"   "PuRd"     "Purples"  "RdPu"    
  #   "Reds"     "YlGn"     "YlGnBu"   "YlOrBr"   "YlOrRd"
  
  if      (colour == 'black') {return('Greys')}
  else if (colour == 'blue') {return('Blues')}
  else if (colour == 'red') {return('Reds')}
  else if (colour == 'green') {return('Greens')}
  else if (colour == 'orange') {return('Oranges')}
  else if (colour == 'purple') {return('Purples')}
  else {return(colour)}
} 

# Private method: Do not export
arg.verify = function(weighting, metric){
  assert(metric %in% metrics, "Error from get.dist(): Argument metric is unknown")
  assert(weighting %in% weightings, "Error from get.dist(): Argument weighting is unknown")
}


defset = list(remove_punctuation = TRUE, remove_numbers = TRUE,
              tolower = TRUE, metric = 'spherical', stemming = TRUE,
              remove_special_characters = TRUE, plain_text = TRUE,
              unique = TRUE, as_matrix = TRUE, 
              weighting = 'tfidf', wc_max_words = 50,
              wc_rot_per = 0.4,
              wc_colour = 'blue', num_clust = 3, wc_gradient = 'freq',
              plot_colour = 'blue', sparsity = 0.99)

TEXT.MINER <- setRefClass("TEXT.MINER", 
                             fields = list(
                               text          = "character",
                               n.text        = "numeric",
                               stop.words    = "character",
                               dictionary    = "data.frame",
                               wrds          = "character",
                               arr.time      = "POSIXlt",
                               settings      = "list",
                               
                               DTM           = "matrix",
                               W.tfidf       = "matrix",
                               W.bin         = "matrix",
                               
                               D.bin         = "matrix",
                               
                               D.freq.euc    = "matrix",
                               D.freq.max    = "matrix",
                               D.freq.man    = "matrix",
                               D.freq.can    = "matrix",
                               D.freq.min    = "matrix",
                               D.freq.sph    = "matrix",
                               
                               D.tfidf.euc   = "matrix",
                               D.tfidf.max   = "matrix",
                               D.tfidf.man   = "matrix",
                               D.tfidf.can   = "matrix",
                               D.tfidf.min   = "matrix",
                               D.tfidf.sph   = "matrix",
                               
                               C             = "numeric",
                               CRS           = "matrix",
                               CRS.dist      = "matrix",
                               CR            = "numeric",
                               CR.dist       = "numeric"
                             ),
                          
                             methods = list(
                               initialize = function(text_vect, arr_time = NA, stop_words = stopwords('english'), dictionary = data.frame(), settings = defset){
                                 if (class(text_vect) == 'data.frame'){
                                   text_vect = text_vect[,1]
                                 }
                                 assert(class(text_vect) == 'character', "Argument 'text_vect' must be of class 'character'", match.call()[[1]])
                                 # Check Input Arguments:
                                 
                                 # Replacement:
                                 
                                 if (settings$remove_special_characters){text_vect = remove.special.charachters(text_vect)}
                                 if (settings$unique){text_vect = unique(text_vect)}
                                 
                                 text       <<- text_vect
                                 
                                 n.text     <<- length(text_vect)
                                 
                                 stop.words <<- stop_words
                                 
                                 dictionary <<- dictionary
                                 
                                 settings   <<- settings
                                 
                                 if (!is.na(arr_time)){arr.time <<- arr_time}
                                 
                                 reset()
                                 
                               }, 

                               clust = function(nc = settings$num_clust, weighting = settings$weighting, metric = settings$metric){
                                   W = get.weight.matrix(weighting)
                                   if      (metric == 'euclidean'){
                                     S = kmeans(W, nc)
                                     C         <<- S$cluster
                                     CRS       <<- S$centers
                                   } 
                                   else if (metric == 'spherical'){
                                     library(skmeans)
                                     S = skmeans(W, k = nc)
                                     C         <<- S$cluster    
                                     CRS       <<- S$prototypes
                                   } 
                                   else if (metric %in% c("maximum", "manhattan", "canberra", "binary" , "minkowski")) {
                                     MDS = get.mds(n.dim = min(dim(W)[1], dim(W)[2]), weighting = weighting, metric = metric)
                                     S   = kmeans(MDS, nc)
                                     C    <<- S$cluster    
                                     CRS  <<- S$centers}
                                   else {assert(F, "Error from clust(): metric not supported!")}
                                   return(C)
                               },
                               
                               subsets = function(k){
                                 assert(length(k) == length(text),'Error: length of k must equal num text')
                                 
                                 clusts = list()
                                 for (i in 1:max(k)){
                                   tmr_i  = new('TEXT.MINER', text_vect = text[k == i], dictionary = dictionary, stop_words = stop.words, settings = settings)
                                   clusts = c(clusts, tmr_i)
                                 }
                                 return(clusts)
                               },
                               
                               cluster = function(cn){
                                 if (is.empty(C)){
                                   return()
                                 }
                                 assert (cn <= max(C), 'Error: cn can not be greater than the number of clusters')
                                 assert (length(C) == length(text), 'Error: count of clusters do not match count of texts')
                                 return(subset(C == cn))
                               },

                               clusters = function(){
                                 if (is.empty(C)){
                                   return()
                                 }
                                 return(subsets(C))
                               },
                               
                               subset = function(ti){
                                 tmr  = new('TEXT.MINER', text_vect = text[ti], dictionary = dictionary, stop_words = stop.words, settings = settings)
                                 return(tmr)
                               },
                               
                               set.cluster = function(cn){
                                 if (is.empty(C)){
                                   reset.clusters()
                                 }
                                 assert (cn < max(C) + 2, 'Error: cn can not be greater than the number of clusters')
                                 
                                 dev.off()
                                 plot.2d()
                                 
                                 MDS = get.mds()
                                 ss  = identify(MDS, plot = F)
                                 C[ss] <<- cn
                                 
                                 plot.2d()
                               },
                               
                               set.metric = function(m){
                                 assert(m %in% metrics, "Error: Given metric is unknown!")
                                 settings$metric <<- m
                                 CRS       <<- matrix()
                                 CRS.dist  <<- matrix()
                                 CR        <<- numeric()
                                 CR.dist   <<- numeric()
                               },
                               
                               set.weighting = function(w){
                                 assert(w %in% weightings, "Error: Given weighting is unknown!")
                                 settings$weighting <<- w
                                 CRS       <<- matrix()
                                 CRS.dist  <<- matrix()
                                 CR        <<- numeric()
                                 CR.dist   <<- numeric()
                               },

                               reset.clusters = function(){
                                 C <<- rep(1, length(text))
                               },
                               
                               reset.settings = function(){
                                 settings <<- default.settings
                               },
                               
                               get.weight.matrix = function(weighting = settings$weighting){
                                 if      (weighting == 'freq'){return(get.docterm())} 
                                 else if (weighting == 'tfidf'){return(get.tfidf())} 
                               },
                                 
                               get.docterm = function(){
                                 if (is.empty(DTM)){
                                   crp = Corpus(VectorSource(text))

                                   if (settings$remove_punctuation){crp <- tm_map(crp, removePunctuation)}
                                   if (settings$remove_numbers){crp <- tm_map(crp, removeNumbers)}
                                   if (settings$tolower){crp <- tm_map(crp, content_transformer(tolower))}  #convert to lower case
                                   # remove standard English stopwords, extra stopwords, 
                                   if (length(stop.words) != 0){crp <- tm_map(crp, removeWords, stop.words)}
                                   if (settings$plain_text){crp <- tm_map(crp, PlainTextDocument)} # make sure it's read as plain text
                                   if (settings$stemming){
                                     library(SnowballC)
                                     crp <- tm_map(crp, stemDocument)
                                   }
                                   # Make dictionary conversions
                                   if (inherits(dictionary,'data.frame')){
                                     if (dim(dictionary)[1] > 0){                                     
                                       for (j in seq(crp)){
                                         for (i in 1:(dim(dictionary)[1])){
                                           crp[[j]]$content <- gsub(paste0('\\<', dictionary[i,1] , '\\>'), dictionary[i,2], crp[[j]]$content)  
                                         }
                                       }
                                     }
                                   }
                                   dtm = DocumentTermMatrix(crp, control = list(minWordLength = 1))
                                   rownames(dtm) = c()
                                   dtm <- removeSparseTerms(dtm, settings$sparsity)
                                   if (dim(dtm)[2] == 0){
                                     W.tfidf <<- matrix()
                                     W.bin   <<- matrix()
                                     wrds    <<- character()
                                     return(DTM)
                                   }
                                   DTM <<- as.matrix(dtm)
                                   # Remove texts with total zero frequency:
                                   r       = rowSums(DTM)
                                   zeros   = which(r == 0)
                                   if (length(zeros) > 0){
                                     dtm     = dtm[- zeros,]
                                     text    <<- text[- zeros]
                                     DTM     <<- DTM[- zeros,]
                                   }
                                   if (dim(DTM)[1] == 0){
                                     DTM     <<- matrix()
                                     W.tfidf <<- matrix()
                                     W.bin   <<- matrix()
                                     wrds    <<- character()
                                     return(DTM)
                                   }
                                   W.tfidf <<- as.matrix(weightTfIdf(dtm, normalize = TRUE))
                                   W.bin   <<- as.matrix(weightBin(dtm))
                                  
                                   wrds    <<- colnames(DTM)
                                 }
                                 return (DTM)
                               },
                               
                               get.tfidf = function(){
                                 if (is.empty(W.tfidf)){D = get.docterm()}
                                 return(W.tfidf)
                               }, 
                               
                               get.dist = function(weighting = settings$weighting, metric = settings$metric){
                                 arg.verify(weighting, metric)
                                 
                                 if (metric == 'binary'){return(dist.binary())}
                                 else if (weighting == 'freq'){
                                   if      (metric == 'euclidean'){return(dist.freq.euclidean())}
                                   else if (metric == 'maximum'){return(dist.freq.maximum())}
                                   else if (metric == 'manhattan'){return(dist.freq.manhattan())}
                                   else if (metric == 'canberra'){return(dist.freq.canberra())}
                                   else if (metric == 'minkowski'){return(dist.freq.minkowski())}
                                   else if (metric == 'spherical'){return(dist.freq.spherical())}
                                   else {assert(F, "Error: Not Supported !")}
                                 } else if (weighting == 'tfidf'){
                                   if      (metric == 'euclidean'){return(dist.tfidf.euclidean())}
                                   else if (metric == 'maximum'){return(dist.tfidf.maximum())}
                                   else if (metric == 'manhattan'){return(dist.tfidf.manhattan())}
                                   else if (metric == 'canberra'){return(dist.tfidf.canberra())}
                                   else if (metric == 'minkowski'){return(dist.tfidf.minkowski())}
                                   else if (metric == 'spherical'){return(dist.tfidf.spherical())}
                                   else {assert(F, "Error: Not Supported !")}
                                 }
                               },
                               
                               get.mds  = function(n.dim = 2, weighting = settings$weighting, metric = settings$metric){
                                 D = get.dist(weighting = weighting, metric = metric)
                                 return(cmdscale(D, n.dim))
                               },
                               
                               get.weights    = function(weighting = settings$weighting){
                                 if (weighting == 'tfidf'){W = get.tfidf()}
                                 else if (weighting == 'freq'){W = get.docterm()}
                                 else {assert(F, "Error: Not Supported!")}
                                 return(colSums(W))
                               },
                               
                               reset = function(){
                                 
                                 # Resets the text miner:
                                 # Erases all the analysis
                                 
                                 DTM         <<- matrix()
                                 W.tfidf     <<- matrix()
                                 W.bin       <<- matrix()
                                 wrds        <<- character()
                                 
                                 D.bin       <<- matrix()
                                 
                                 D.freq.euc  <<- matrix()
                                 D.freq.sph  <<- matrix()
                                 D.freq.max  <<- matrix()
                                 D.freq.man  <<- matrix()
                                 D.freq.can  <<- matrix()
                                 D.freq.min  <<- matrix()
                                 
                                 D.tfidf.euc  <<- matrix()
                                 D.tfidf.max  <<- matrix()
                                 D.tfidf.man  <<- matrix()
                                 D.tfidf.can  <<- matrix()
                                 D.tfidf.min  <<- matrix()
                                 D.tfidf.sph  <<- matrix()
                                 
                                 C          <<- numeric()
                                 CRS        <<- matrix()
                                 CRS.dist   <<- matrix()
                                 CR         <<- numeric()
                                 CR.dist    <<- numeric()
                               },
                               
                               dist.binary = function(){
                                 if (is.empty(D.bin)){
                                   D.bin <<- as.matrix(dist(get.docterm(), method = "binary"))
                                 }
                                 return (D.bin)
                               },
                               
                               dist.freq.euclidean = function(){
                                 if (is.empty(D.freq.euc)){
                                   D.freq.euc <<- as.matrix(dist(get.docterm(), method = "euclidean"))
                                 }
                                 return (D.freq.euc)
                               },
                               
                               dist.freq.maximum = function(){
                                 if (is.empty(D.freq.max)){
                                   D.freq.max <<- as.matrix(dist(get.docterm(), method = "maximum"))
                                 }
                                 return (D.freq.max)
                               },
                               
                               dist.freq.manhattan = function(){
                                 if (is.empty(D.freq.man)){
                                   D.freq.man <<- as.matrix(dist(get.docterm(), method = "manhattan"))
                                 }
                                 return (D.freq.man)
                               },
                               
                               dist.freq.canberra = function(){
                                 if (is.empty(D.freq.can)){
                                   D.freq.can <<- as.matrix(dist(get.docterm(), method = "canberra"))
                                 }
                                 return (D.freq.can)
                               },
                               
                               dist.freq.minkowski = function(){
                                 if (is.empty(D.freq.min)){
                                   D.freq.min <<- as.matrix(dist(get.docterm(), method = "minkowski"))
                                 }
                                 return (D.tfidf.min)
                               },

                               dist.freq.spherical = function(){
                                 if (is.empty(D.freq.sph)){
                                   W.norm         = apply(get.docterm(), 1, vect.normalize)
                                   D.freq.sph <<- 1 - t(W.norm) %*% W.norm
                                 }
                                 return (D.freq.sph)
                               },
                               
                               
                               dist.tfidf.euclidean = function(){
                                 if (is.empty(D.tfidf.euc)){
                                   D.tfidf.euc <<- as.matrix(dist(get.tfidf(), method = "euclidean"))
                                 }
                                 return (D.tfidf.euc)
                               },

                               dist.tfidf.maximum = function(){
                                 if (is.empty(D.tfidf.max)){
                                   D.tfidf.max <<- as.matrix(dist(get.tfidf(), method = "maximum"))
                                 }
                                 return (D.tfidf.max)
                               },

                               dist.tfidf.manhattan = function(){
                                 if (is.empty(D.tfidf.man)){
                                   D.tfidf.man <<- as.matrix(dist(get.tfidf(), method = "manhattan"))
                                 }
                                 return (D.tfidf.man)
                               },
                               
                               dist.tfidf.canberra = function(){
                                 if (is.empty(D.tfidf.can)){
                                   D.tfidf.can <<- as.matrix(dist(get.tfidf(), method = "canberra"))
                                 }
                                 return (D.tfidf.can)
                               },
                               
                               dist.tfidf.minkowski = function(){
                                 if (is.empty(D.tfidf.min)){
                                   D.tfidf.min <<- as.matrix(dist(get.tfidf(), method = "minkowski"))
                                 }
                                 return (D.tfidf.min)
                               },
                               
                               dist.tfidf.spherical = function(){
                                 if (is.empty(D.tfidf.sph)){
                                   W.norm         = apply(get.tfidf(), 1, vect.normalize)
                                   D.tfidf.sph <<- 1 - t(W.norm) %*% W.norm
                                 }
                                 return (D.tfidf.sph)
                               },

                               words      = function(){
                                 if (is.empty(wrds)){
                                   D = get.docterm()   
                                 } 
                                 return(wrds)
                               },
                               
                               freq.words = function(freq_threshold = 20){
                                 f = word.freq()
                                 f = f[f > freq_threshold]  
                                 return(names(sort(f, decreasing = TRUE)))
                               },
                               
                               word.freq       = function(){return(colSums(get.docterm()))},
                               
                               word.cloud = function(weighting = settings$weighting){
                                 library(wordcloud)
                                 
                                 v        = sort(get.weights(weighting = weighting), decreasing = T)
                                 rnd.cols = F
                                 cols     = settings$wc_colour
                                 if (settings$wc_gradient %in% c('tfidf', 'freq', 'random')){
                                   if (settings$wc_gradient == 'random'){vp = runif(length(v))} else {vp = sort(get.weights(weighting = settings$wc_gradient), decreasing = T)}
                                   col      = round(vect.map(vp, 1, 9))
                                   pallete  = brewer.pal(9, plural.col(settings$wc_colour)) # blue gradient
                                   cols     = pallete[col]
                                 } else if (settings$wc_gradient == 'none'){cols = rep(settings$wc_colour, length(v))}
                                   else {assert(F, "Error: Not Supported!")}
                                 wordcloud(names(v), v, min.freq = quantile(v)[2], max.words = settings$wc_max_words, 
                                           random.order = F, random.color = rnd.cols, rot.per = settings$wc_rot_per, 
                                           ordered.colors = T, colors = cols)                                 
                               },
                               
                               plot.2d    = function(weighting = settings$weighting, metric = settings$metric){
                                 MDS = get.mds(weighting = weighting, metric = metric)
                                 if (is.empty(C)){clrs = settings$plot_colour} else {clrs = C}
                                 plot(MDS, col = clrs)
                               },

                               rch.plot.2d    = function(weighting = settings$weighting, metric = settings$metric){
                                 MDS = get.mds(weighting = weighting, metric = metric)
                                 if (is.empty(C)){clrs = settings$plot_colour} else {clrs = C}
                                 MDS = as.data.frame(MDS)
                                 MDS = cbind(as.factor(sequence(nrow(MDS))), MDS, as.factor(clrs))
                                 colnames(MDS) <- c('Row', 'X', 'Y', 'Cluster')
                                 MDS$X =  MDS$X - min(MDS$X)
                                 MDS$Y =  MDS$Y - min(MDS$Y)
                                 
                                 dp <- dPlot( Y~X,
                                  groups = c("Row", "Cluster"),
                                  data   = MDS,
                                  type   = "bubble",
                                  height = 380, width  = 590,
                                  bounds = list(x=60, y=30, width=420, height=310),
                                  xlab = "X", ylab = "Y"
                                 )
                                 dp$xAxis(type = "addMeasureAxis")
                                 dp$legend(x = 530,y = 130, width = 60,height = 300, horizontalAlign = "left")
                                 dp$setTemplate(afterScript = gen.dPlot.script(field_name = 'Cluster'))
                                 
                                 return(dp)
                               },
                               
                               plot.3d    = function(weighting = settings$weighting, metric = settings$metric){
                                 library(rgl)
                                 
                                 MDS = get.mds(n.dim = 3, weighting = weighting, metric = metric)
                                 if (is.empty(C)){clrs = settings$plot_colour} else {clrs = C}
                                 plot3d(MDS, col = clrs)
                               },
                               
                               plot.clusters = function(nc = settings$num_clust, weighting = settings$weighting, metric = settings$metric){
                                 library(cluster)
                                 W = get.weight.matrix(weighting = weighting)
                                 if (dim(W)[1] < dim(W)[2]){
                                   W = W[, order(word.freq(), decreasing = T)[1:(dim(W)[1])]]
                                 }
                                 if (is.empty(C)){get.clusters(nc, weighting, metric)}
                                 clusplot(W, C, color=T, shade=T, labels=2, lines=0, cex=0.7)                                 
                               },
                               
                               plot.special = function(){
                                 Motion=gvisMotionChart(Fruits, 
                                                        idvar="Fruit", 
                                                        timevar="Year")
                                 return(Motion)
                               },
                               
                               word.bar   = function(weighting = settings$weighting){
                                 weight <- sort(get.weights(weighting), decreasing=TRUE)   
                                 # Plot word frequencies
                                 wf <- data.frame(word=names(weight), weight=weight)   
                                 
                                 library(ggplot2)   
                                 thr <- quantile(weight)[4]
                                 p <- ggplot(subset(wf, weight > thr), aes(word, weight))    
                                 p <- p + geom_bar(stat="identity")   
                                 p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))   
                                 p                                  },
                               
                               word.dist  = function(weighting = settings$weighting, metric = settings$metric){
                                 W = t(get.weight.matrix(weighting = weighting))
                                 if      (metric == 'binary')   {return(as.matrix(dist(W, method = 'binary')))} 
                                 else if (metric == 'euclidean'){return(as.matrix(dist(W, method = 'euclidean')))}
                                 else if (metric == 'spherical'){
                                   W.norm         = apply(W, 1, vect.normalize)
                                   return(1 - t(W.norm) %*% W.norm)
                                 }
                                 else {assert(F, "Error: Not Supported !")}
                               },
                               
                               center     = function(weighting = settings$weighting, metric = settings$metric){
                                 W = get.weight.matrix(weighting = weighting)
                                 if (metric != 'euclidean'){W = matrix.normalize(W, 2)}
                                 CR <<- colMeans(W)
                                 return(CR)
                               },

                               centers     = function(weighting = settings$weighting, metric = settings$metric){
                                 W  = get.weight.matrix(weighting = weighting)
                                 if (is.empty(C)){return()}
                                 nc = max(C)
                                 if (nc == 0){return()}
                                 nw = dim(W)[2]
                                 CRS <<- zeros(nc, nw)
                                 for (i in 1:nc){
                                   Wi = W[C == i,]
                                   if (metric != 'euclidean'){Wi = matrix.normalize(Wi, 2)}
                                   CRS[i,] <<- colMeans(Wi)
                                 }
                                 colnames(CRS) <<- words()
                                 return(CRS)
                               },
                               
                               centers.dist = function(weighting = settings$weighting, metric = settings$metric){
                                 if (is.empty(CRS)){centers()}
                                 W  = get.weight.matrix(weighting = weighting)
                                 nc = dim(CRS)[1]
                                 nd = dim(W)[1]
                                 CRS.dist <<- zeros(nd, nc)
                                 for (i in 1:nd){
                                   for (j in 1:nc){
                                     CRS.dist[i, j] <<- vect.dist(W[i,], CRS[j,])
                                   }
                                 }
                                 return(CRS.dist)
                               }, 
                               center.dist = function(weighting = settings$weighting, metric = settings$metric){
                                 if (is.empty(CR)){center()}
                                 W  = get.weight.matrix(weighting = weighting)
                                 nd = dim(W)[1]
                                 CR.dist <<- rep(0, nd)
                                 for (i in 1:nd){
                                   CR.dist[i] <<- vect.dist(W[i,], CR)
                                 }
                                 return(CR.dist)
                               }
                               
                               
                             ))

# Some generic functions

setMethod("length", "TEXT.MINER", function(x) length(x$text))

# length is a pre-defined generic function like:
# summary, plot, show, print, ...
# 
# you can define your own generic function name like 
# face(x)

setGeneric("words", function(x) standardGeneric('words'))
setMethod("words", "TEXT.MINER", function(x) {
  if (is.empty(x$wrds)){D = x$get.docterm()} 
  return(x$wrds)
})


# define a setter:
setGeneric("metric<-", function(x, value) standardGeneric("metric<-"))
setReplaceMethod("metric", "TEXT.MINER", function(x, value) {
  assert(value %in% metrics, "Error: Given metric is unknown!")
  x$settings$metric <<- value
  x$CRS       <<- matrix()
  x$CRS.dist  <<- matrix()
  x$CR        <<- numeric()
  x$CR.dist   <<- numeric()
  x
})

# define the validity method:
# setValidity("TEXT.MINER", function(object){
#   if !is.character(objects$settings$metric)
# })


text_mining_lib_v2:

# Header
# Filename:     text_mining_lib_v2.R
# Description:  Contains functions useful for working with texts. A supplementary to "tm" package.
# Author:       Nima Ramezani Taghiabadi
# Email :       Nima.Ramezani@cba.com.au
# Start Date:   21 October 2013
# Last change:  19 November 2015
# Version:      2.0

# This url is helpful to generate a workspace containing all english words
# http://www.manythings.org/vocabulary/lists/l/words.php?f=3esl.08

# Changes to previous version:

# 1- function text.vect.to.term.document.matrix() Applies a given conversion dictionary before creating the corpus


library(tm)
library(stringr)

url.parts <- function(x) {
  ## returns parts of a URL:
  m <- regexec("^(([^:]+)://)?([^:/]+)(:([0-9]+))?(/.*)", x)
  parts <- do.call(rbind,
                   lapply(regmatches(x, m), `[`, c(3L, 4L, 6L, 7L)))
  colnames(parts) <- c("protocol","host","port","path")
  parts
}

remove.special.charachters <- function(str.vect){
  return(gsub("[[:punct:]]", " ", str.vect))
}

text.vect.to.term.document.matrix = function(tv, extra_stopwords = c(), unique = TRUE){
  #Converts a vector of texts into a frequency matrix 
  #Step 1: remove special characters from the given text vector
  tv = remove.special.charachters(tv)
  if (unique){tv = unique(tv)}
  #Step 2: construct a corpus out of the given text vector
  crp = Corpus(VectorSource(tv))
  #Step 2: make necessary modifications
  stoplist=c(stopwords('english'), letters, extra_stopwords)
  tdm = TermDocumentMatrix(crp,control = list(removePunctuation = TRUE, stopwords = stoplist,removeNumbers = TRUE, tolower = TRUE,stemming = TRUE))
  return(tdm)  
}

text.vect.to.document.term.matrix = function(tv, extra_stopwords = c(), unique = TRUE, dictionary = NA){
  #Converts a vector of texts into a frequency matrix 
  #Step 1: remove special characters from the given text vector
  tv = remove.special.charachters(tv)
  if (unique){tv = unique(tv)}
  # Step 2: construct a corpus out of the given text vector
  crp = Corpus(VectorSource(tv))

  # Step 3: Clean the text from punctuations and numbers and convert all to lower case
  crp <- tm_map(crp, removePunctuation) # remove punctuation
  crp <- tm_map(crp, removeNumbers) # remove numbers
  crp <- tm_map(crp, content_transformer(tolower)) #lower case
  
  # Step 3: Make dictionary conversions
  if (inherits(dictionary,'data.frame')){
    for (j in seq(crp)){
      for (i in 1:(dim(dic)[1])){
        crp[[j]]$content <- gsub(paste0('\\<', dic[i,1] , '\\>'), dic[i,2], crp[[j]]$content)  
      }
    }
  }
  
  #Step 4: remove standard English stopwords, extra stopwords, 
  stoplist=c(stopwords('english'), letters, extra_stopwords)
  crp <- tm_map(crp, removeWords, stoplist)
  crp <- tm_map(crp, PlainTextDocument) # make sure it's read as plain text

  #crp <- tm_map(crp, stemDocument)
  
  # Step 5: Do standard English stemming and convert to Term Document Matrix 
  
  dtm <- DocumentTermMatrix(crp, control = list(minWordLength = 1))  
  
  return(dtm)  
}

text.vect.to.frequency.matrix = function(tv, extra_stopwords = c(), unique = TRUE, dictionary = NA){
  #Converts a vector of texts into a frequency matrix 
  tdm = text.vect.to.term.document.matrix(tv, extra_stopwords = extra_stopwords, unique = unique, dictionary = dictionary)  
  return(t(as.matrix(tdm)))
}

sensitivity <- function(predicted.class, actual.class) {
  return(mean(predicted.class[actual.class]))
}

naive.bayes.test <- function(data){

  #step 1: extract positive and negative documents
  pn.index   = which(data$sentiment == "positive" | data$sentiment == "negative")
  texts      = data$text[pn.index]
  sentiments = data$sentiment[pn.index]
  # Step 2: Make a frequency matrix out of the text vector  
  A = text.vect.to.frequency.matrix(texts)
  # Step 3: Split train and validate texts randomly
  n.texts = dim(A)[1]
  train.index = sample(n.texts, floor(n.texts/2))
  A.train = A[train.index,]
  C.train = sentiments[train.index]
  A.validate = A[- train.index,]
  C.validate = sentiments[- train.index]
  # Step 4: Find the bias for the training model
  p.positive = mean(C.train == "positive")
  p.negative = 1.0 - p.positive
  bias = log(p.positive/p.negative)
  # Step 5: Find sentiment weightings for the words:
  
  #   Step 5.0: change train frequencies according to the rule of success:
  A.train = A.train + 1
  #   Step 5.1: Find the word frequencies in positive texts
  word.freqs.in.positive.texts = colSums(A.train[C.train=="positive",])
  total.number.of.words.in.positive.texts =sum(word.freqs.in.positive.texts)
  #   Step 5.2: Find the word probabilities in positive texts
  word.probs.positive = word.freqs.in.positive.texts/total.number.of.words.in.positive.texts

  #   Step 5.3: Find the word frequencies in negative texts
  word.freqs.in.negative.texts = colSums(A.train[C.train=="negative",])
  total.number.of.words.in.negative.texts =sum(word.freqs.in.negative.texts)
  #   Step 5.2: Find the word probabilities in negative texts
  word.probs.negative = word.freqs.in.negative.texts/total.number.of.words.in.negative.texts
  #   Step 5.3: Find the word weights
  word.weights = log(word.probs.positive/word.probs.negative)
  
  # Step 6: Compute sentiments of validation data
  
  scores=c()
  n.texts = length(C.validate)
  for(i in 1:n.texts){
     word.index = which(A.validate[i, ] > 0)
     scores[i] = sum(word.weights[word.index]) + bias
  }
  
  mid.score  = median(scores)
  sentiments.computed = (scores > mid.score)
  sentiments.observed = (C.validate == "positive") 
  
  # Step 7: Statistical Analysis
  num.suc.preds = n.texts - sum(xor(sentiments.computed, sentiments.observed))
  suc.rate = num.suc.preds/n.texts
  sd.proportion = sqrt(suc.rate*(1.0 - suc.rate)/n.texts)
  conf.int = range(suc.rate - 1.96*sd.proportion, suc.rate + 1.96*sd.proportion)
  # Step 8: Compute Sensitivity and Specificity
  sensitive = mean(sentiments.computed[sentiments.observed])
  specific  = mean(!sentiments.computed[!sentiments.observed])
  # Step 9: Issue the output
  output = list(number.of.successful.predictions = num.suc.preds, out.of = n.texts, success.rate = suc.rate, confidence.interval = conf.int, sensitivity=sensitive, specificity = specific)
  return(output)
}
 

# binary.bayesian.classification.model <- function(training.text.vector, training.sentiments)
#   # This function generates a binary bayesian classification model from thegiven taining data
#   # Input 1: training.text.vector - A vector of texts (characters) containing the texts used for training the model
#   # Input 2: training.sentiments - A vector of booleans containing TRUE and FALSE as a binary outcome of its equivalent text
#   # Input 1 & Input 2 must have the same length. The following piece of code checks this:
#   if (!vector.dimension.equal(training.text.vector, training.sentiments)){
#     print("binary.bayesian.classification.model Error: Given vectors must have the same length")
#     return(NA)
#   }
#   # Output: A list of variables:
#   # Output$words: a vector of strings containing all the words in the given documents
#   # Output$weightings: a vector of conditional probabilities P(X = x_i|C = 1) for each word
#   # output$bias: bias of the model
#   
#   p.C.1 = mean(training.sentiments)
#   p.c.0 = 1 - p.c.1
#   
#   #Convert the training text vector into a frequency matrix:
#   A = str.vect.to.frequency.matrix(training.text.vector)
#   n.words = ncols(A)
#   # this function is not complete yet. Complete it in the future
# 
# predict.sentiment <- function(bin.bayes.model, text)
#   #Input 1: bin.bayes.model - the output of binary.bayesian.classification.model
#   #Input 2: text - document that you want to predict its sentiment
#   # this function is not complete yet. Complete it in the future
#   

binary.metric <- function(x, y) {
  return(mean(xor(x, y)))
}

text.distance <- function(text, train.texts) {
  # text is a row vector from A.validate, train.texts is A.train
  
  # return a vector of distances between text and all vectors in
  # train.texts measured using the function binary.metric
  td=c()
  n.texts=dim(train.texts)[1]
  for (i in 1:n.texts){
    td[i] = binary.metric(text, train.texts[i,])
  }
  return(td)
}

kNN.classify <- function(text, train.texts, train.classes, k = 10) {
  # identify the k closest texts
  d = text.distance(text, train.texts)
  closest.k.text.positions = order(d)[1:k]
  
  # select classes of closest texts
  close.classes = train.classes[closest.k.text.positions]
  
  # return the most frequently appearing class (the mode)
  t = table(close.classes)
  return(names(which.max(t)))
}

kNN.predict.classes <- function(validate.texts, train.texts, train.classes, k = 10){
  
  n.texts=dim(validate.texts)[1]
  classes=c()
  for(i in 1:n.texts){
    print(i)
    classes[i] = kNN.classify(validate.texts[i,], train.texts, train.classes, k = k)
  }
  return(classes)
}

test.kNN.classifier <- function(data, k = 10){
  
  #step 1: extract positive and negative documents
  pn.index   = which(data$sentiment == "positive" | data$sentiment == "negative")
  texts      = data$text[pn.index]
  sentiments = data$sentiment[pn.index]
  # Step 2: Make a frequency matrix out of the text vector  
  A = text.vect.to.frequency.matrix(texts)
  # Step 3: Split train and validate texts randomly
  n.texts = dim(A)[1]
  train.index = sample(n.texts, floor(n.texts/2))
  A.train = A[train.index,]
  C.train = sentiments[train.index]
  A.validate = A[- train.index,]
  C.validate = sentiments[- train.index]
  # Step 4: Classify the training data
  predicted.classes = kNN.predict.classes(A.validate, A.train, C.train, k = k)

  sentiments.computed = (predicted.classes == "positive")
  sentiments.observed = (C.validate == "positive") 
  
  # Step 5: Statistical Analysis
  n.texts = length(C.validate)
  num.suc.preds = n.texts - sum(xor(sentiments.computed, sentiments.observed))
  suc.rate = num.suc.preds/n.texts
  sd.proportion = sqrt(suc.rate*(1.0 - suc.rate)/n.texts)
  conf.int = range(suc.rate - 1.96*sd.proportion, suc.rate + 1.96*sd.proportion)
  # Step 8: Compute Sensitivity and Specificity
  sensitive = mean(sentiments.computed[sentiments.observed])
  specific  = mean(!sentiments.computed[!sentiments.observed])
  # Step 9: Issue the output
  
  output = list(number.of.successful.predictions = num.suc.preds, out.of = n.texts, success.rate = suc.rate, confidence.interval = conf.int, sensitivity=sensitive, specificity = specific)
  return(output)
}


SimpleWordle <- function(words, freq, min.freq=10) {
  keep <- (freq >= min.freq)
  words <- words[keep]
  freq <- freq[keep]
  
  ord <- order(freq, decreasing=TRUE)
  freq <- freq[ord]
  words <- words[ord]
  
  plot.new()
  op <- par(mar=rep(0,4))
  plot.window(c(-1,1),c(-1,1), asp=1)
  
  smin <- 0.5
  smax <- 4
  sizes <- smin + (smax-smin) *(freq-min(freq))/(max(freq)-min(freq))
  
  thetaStep <- 0.1
  rStep <- 0.05*thetaStep/(2*pi)
  boxes <- list()
  
  box <- function(r, theta, word, size) {
    wid <- strwidth(word, cex=size)
    ht <- strheight(word, cex=size)
    x <- r*cos(theta)
    y <- r*sin(theta)
    return(c(x-wid/2, x+wid/2, y-ht/2, y+ht/2))
  }
  
  is.overlapped <- function(box, boxes) {
    if(length(boxes)==0) return(FALSE)
    for(i in 1:length(boxes)) {
      boxi <- boxes[[i]]
      if(boxi[1]>box[2]  || boxi[2]<box[1] || boxi[3]>box[4] || boxi[4] < box[3]) next
      else return(TRUE)
    }
    return(FALSE)
  }
  r <- 0
  theta <- 0
  for(i in 1:length(words)) {
    repeat {
      b<-box(r, theta, words[i], sizes[i])
      if(!is.overlapped(b, boxes)) break
      theta <- theta + thetaStep
      r <- r + rStep
    }
    text(r*cos(theta),r*sin(theta), words[i], adj=c(0.5,0.5), cex=sizes[i])
    boxes <- c(list(b), boxes)
  }
  par(op)
  invisible() 
}

